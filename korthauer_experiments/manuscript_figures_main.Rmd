---
title: "Manuscript Figures for Genome Biology"
author: "Rafalab"
output: 
    html_document:
        toc: true
        toc_float: true
        highlight: tango
        number_sections: true
---
# BiocManager::install("SummarizedBenchmark")
# devtools::install_github("areyesq89/SummarizedBenchmark", ref = "fdrbenchmark")
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This Rmd generates the main manuscript figures. Additional Rmds
generate the supplementary case study and in silico experiment figures 
(`manuscript_figures.Rmd`) and supplementary case study figures 
(`manuscript_figures_sims.Rmd`). 

# Set up workspace 

```{r workspacesetup}
# Load packages and source benchmark FDR
library(tidyr)
library(dplyr)
library(ggplot2)
library(magrittr)
library(cowplot)
library(tibble)
library(ggthemes)
library(grid)
library(SummarizedBenchmark)
library(ggrepel)

## load helper functions
for (f in list.files("../datasets/R", "\\.(r|R)$", full.names = TRUE)) {
    source(f)
}

# Assumes sb objects for the case studies and in silico experiments are in 
# the following location, which contains subfolders
# for each casestudy (if this isn't true, then parsing the case study and 
# dataset names later on will be incorrect)
path <- "../IncludeNewMethods/casestudy/casestudy-rds/"
#path2 <- "../IncludeNewMethods/casestudy/glm_new/"
path2 <- "../IncludeNewMethods/casestudy/gmm_final_v2/"
path3 <- "../IncludeNewMethods/casestudy/gmm_neural/"

# Also assumes that simulation results summary file is in the following location
# The `result-metrics.rds` was generated in the 
# `datasets/simulations/simulations-summary.Rmd` file.
resmet_file <- file.path("..", "datasets", "simulations", "results-summary", "result-metrics.rds")
gmm_resmet_file <- file.path("..", "datasets", "simulations", "results-summary", "gmm-result-metrics.rds")

# set up results directory
outdir <- "../figures/"
dir.create(outdir, showWarnings = FALSE)

# set alpha cutoff for plots that are fixed at a certain value of alpha
alpha.thresh <- 0.05

# methods to include in all figures ( exclude bonferroni and fdrreg-e)
methodset <- c("bh", "ihw", paste0("ihw-a0", 1:9), "ihw-a10", 
               "qvalue", "bl", "bl-df03", "lfdr",
               "fdrreg-t", "ashq", "adapt-glm")
```


We use the standardize candy color scheme and line types for the plots. We'll
add the "lfdr*" method, which indicates lfdr was applied with fewer than 200 
tests per bin (out of 20 bins).

```{r}
col <- as.character(candycols$col)
names(col) <- as.character(candycols$Method)
lty <- as.character(candycols$lty)
names(lty) <- as.character(candycols$Method)
```

To generate the figures in this document, the simulation results must first be aggregated by running the code at `datasets/simulations/simulations-summary.Rmd`.

```{r load-sim-metrics}
resmet <- readRDS(resmet_file)
gmm_resmet <- readRDS(resmet_file)
resmet <- rbind(resmet,gmm_resmet)
```

Unfortunately, because the plots in this section illustrate FDR and TPR acorss varying simulation settings rather than varying nominal FDR cutoff, we cannot simply use the `plotsim_average()` function to generate plots. Instead, we define the following helper function to consistently generate plots similar to those output by `plotsim_average()` with arbitrary covariate, e.g. number of tests or proportion of null hypotheses, as the x-axis. The function assumes that the input table only includes FDR or TPR values for each method at a single nominal alpha cutoff. In these figures, we only plot FDR and TPR values at the nominal alpha cutoff of 0.05.

```{r genplot-function}
genplot <- function(tab, cov, type = c("info", "diff"), xt = "", met = c("FDR", "TPR"), ebw = 0.0025) {
    met <- match.arg(met)
    cov <- rlang::enquo(cov)
    type <- match.arg(type)
    if (type == "info") {
        ymean <- rlang::quo(mean.info)
        yse <- rlang::quo(se.info)
    } else if (type == "diff") {
        ymean <- rlang::quo(mean.diff)
        yse <- rlang::quo(se.diff)
    }        
    gp <- dplyr::filter(tab, performanceMetric == met) %>%
        ggplot(aes(x = !!cov, y = !!ymean, color = Method, group = Method)) +
        geom_line(aes(linetype = Method), alpha = 0.85) +
        geom_errorbar(aes(ymin = !!ymean - !!yse, ymax = !!ymean + !!yse), width = ebw, alpha=0.5) +
        scale_linetype_manual(values = lty) + 
        scale_color_manual(values = col) +
        expand_limits(y = 0) +
        theme_classic() +
        theme(axis.title = element_text(face = "bold", size = 11),
              plot.title = element_text(face = "bold", size = 12))
    if (type == "diff") {
        gp <- gp + geom_hline(yintercept = 0, lty = 2, color = "blue", alpha = 1/2)
        gp <- gp + scale_y_continuous(bquote(Delta ~ .(met) ~ (informative-uninformative)),
                                      labels = scales::percent)
    } else {
        gp <- gp + scale_y_continuous(met, labels = scales::percent)
        if (met == "FDR") {
            if (rlang::quo_name(cov) == "alpha") {
                gp <- gp + geom_abline(lty = 2, color = "blue", alpha = 1/2) 
            } else {
                gp <- gp + geom_hline(yintercept = 0.05, lty = 2, color = "blue", alpha = 1/2) 
            }
        }
    }
    gp
}
```

# Figure 1 - Methods summary table

This figure is created manually using performanceMetricnote.

# Figure 2 - FDR

This figure contains the main FDR results for the simulations and *in silico* 
experiments. Panel (A, left) contains the FDR results for a representative setting of
the yeast *in silico* experiments, showing that all methods control FDR. Panel 
(A, right) contains the FDR results for the polyester read count simulation, showing that
most methods control the FDR, with the exception of lfdr and ash, which have slightly
inflated FDR. Panels (B) contains the FDR results for simulations that 
vary the number of hypotheses and the proportion of null hypotheses, respectively. 
These results show that ash and lfdr fail to control FDR under certain settings. 
# DO NOT RUN HERE, RUN LOCAL
# DO NOT RUN HERE, RUN LOCAL
# DO NOT RUN HERE, RUN LOCAL
```{r Figure2, fig.width = 7.5, fig.height = 6.5}
# DO NOT RUN HERE, RUN LOCAL
# DO NOT RUN HERE, RUN LOCAL
# DO NOT RUN HERE, RUN LOCAL
# FDR in in silico experiments across alpha values 
## Figure 2A: FDR of methods in representative yeast setting at varying alpha levels
objects  <- list.files( path, recursive=TRUE, pattern="rds", full.names=TRUE )


resfile <- objects[grepl("yeast-", objects) & 
                   grepl("de5", objects) & 
                  !grepl("uninf", objects)] 

de5 <- readRDS(file=resfile)

res5d <- plotsim_standardize(de5, alpha = seq(0.01, 0.10, 0.01)) %>% 
  filter(blabel %in% methodset)
p2a <- plotsim_average(res5d, met="FDR",
                merge_ihw = TRUE, errorBars=TRUE) +
      ggtitle("Resampling with spike-ins") +
      scale_x_continuous(expression(paste(bold(alpha), bold(" level"))),
                         breaks=seq(0, 1, by=0.02)) +
      scale_y_continuous("FDR", labels=scales::percent, breaks=seq(0, 1, by=0.02)) +
      expand_limits(y=0.11, x = 0.105) +
      theme(plot.title = element_text(face = "bold", size = 12)) 

## Figure 2B: FDR of methods in representative polyester setting at varying alpha levels
objects <- list.files( path, recursive=TRUE, pattern="rds", full.names=TRUE )
resfile <- objects[grepl("polyester-", objects) & 
                   grepl("de5", objects) & 
                  !grepl("uninf", objects)] 
de5 <- readRDS(file=resfile)
res5dp <- plotsim_standardize(de5, alpha = seq(0.01, 0.10, 0.01)) %>% 
  filter(blabel %in% methodset)
datsub <- res5dp %>% filter(performanceMetric == "FDR",
                                        abs(alpha-0.10) < 1e-6,
                                        blabel %in% c("lfdr", "ashq")) %>%
                                 group_by(blabel, alpha) %>%
                                 summarize(value = mean(value)) %>%
                                 mutate(Method = blabel) 
p2b <- plotsim_average(res5dp, met="FDR",
                merge_ihw = TRUE, errorBars=TRUE) +
      ggtitle("Polyester count simulation") +
      scale_x_continuous(expression(paste(bold(alpha), bold(" level"))),
                         breaks=seq(0, 1, by=0.02)) +
      scale_y_continuous("", labels=scales::percent, breaks=seq(0, 1, by=0.02)) +
      expand_limits(y=0.11, x = 0.105) +
      theme(plot.title = element_text(face = "bold", size = 12)) + 
      theme(legend.position="none") +
      geom_label_repel(data=datsub,
                 aes(alpha, value, label = Method),
                 min.segment.length = unit(0, 'lines'),
                 nudge_x = c(-0.01, 10), nudge_y = c(0,0.01),
                 label.padding = 0.1, 
                 box.padding = 0.1, size = 2.25)
  

p2abTitle <- ggdraw() +
    draw_label(expression(paste(bold("FDR control in RNA-seq "), 
                                bolditalic("in silico"), bold(" experiments"))))

# Panels C and D -> FDR in sims across settings at a single specified alpha
## Figure 2C: FDR of methods across varying number of tests at alpha.thresh
res_ntests <- dplyr::filter(resmet, setting == "varyingntests", 
                            alpha == alpha.thresh, Method %in% methodset) 
p2c <- genplot(res_ntests %>% filter(ntests >= 4000 | Method != "lfdr"), 
               cov = ntests, met = "FDR", ebw = 0.1) +
    scale_x_continuous("Number of tests", trans = "log10",
                       breaks = c(1e2, 5e2, 1e3, 5e3, 1e4, 5e4),
                       labels = c("100", "500", "1e3", "5e3", "1e4", "5e4")) +
    guides(color = FALSE, linetype = FALSE) +
    scale_y_continuous("FDR", labels = scales::percent, breaks=seq(0,1,by=0.02)) +
    ggtitle("Varying number of tests") +
    coord_cartesian(ylim = c(0,0.12)) + 
    geom_line(data = res_ntests %>% 
                filter(ntests <= 5000 & Method == "lfdr", performanceMetric == "FDR"),
              aes(x = ntests, y = mean.info), 
              linetype = "dashed", color = "dodgerblue3", alpha = 0.88) +
    geom_errorbar(data = res_ntests %>% 
                 filter(ntests <= 5000 & Method == "lfdr", performanceMetric == "FDR"),
                 aes(ymin = mean.info - se.info, ymax = mean.info + se.info), width = 0.1, alpha=0.5)

## Figure 2D: FDR of methods across varying null proportion at alpha.thresh
res_pi0 <- dplyr::filter(resmet, setting == "varyingpi0", 
                         alpha == alpha.thresh, Method %in% methodset,
                         pi0 >= 10) # restrict to pi0 at least 10%
p2d <- genplot(res_pi0, cov = 100-pi0, met = "FDR", ebw = 2.5) +
    scale_x_continuous("Proportion non-null",
                       breaks = 100 - c(seq(10, 90, by = 10), 95, 99)) + 
  guides(color = FALSE, linetype = FALSE) + 
  ggtitle("Varying non-null proportion") +
  scale_y_continuous("", labels = scales::percent, breaks=seq(0,1,by=0.02), 
                       limits=c(0,.12)) 

p2cdTitle <- ggdraw() +
    draw_label(expression(paste(bold("FDR across simulation settings ("), 
                                bold(alpha), bold(" = 0.05)"))))

## pull Figure 2 together
top <- plot_grid(p2a + theme(legend.position="none"), p2b, nrow = 1)
bot <- plot_grid(p2c, p2d, nrow = 1)
Fig2 <- plot_grid(p2abTitle, top, p2cdTitle, bot, nrow=4, 
                  rel_heights = c(0.1, 1, 0.1, 1),
                  labels = c("A", "", "B", ""))
Fig2 <- plot_grid(Fig2, get_legend(p2a), rel_widths = c(1, .2))
Fig2
ggsave(file.path(outdir, "Figure2.pdf"), width=7.5, height=6.5)
```

# Figure 3 - Power

This figure contains the main TPR results for the simulations and *in silico* 
experiments, and is analagous to Figure 2. 
Panel (A) contains the TPR results for a representative setting of
the yeast *in silico* experiments and polyester simulations, showing that in 
general, modern FDR-controlling methods have modestly higher power than classic methods. 
In addition, lfdr, fdrreg-t, and adapt tend to have highest power compared to other
methods.
Panel (B) contains the TPR results for simulations that 
vary the number of hypotheses and the proportion of null hypotheses, respectively. 
These results show that adapt and lfdr are very conservative under some settings 
of the number of tests, and that the relative ranking of methods is somewhat 
robust under varying proportions of null hypotheses. 

```{r Figure3, fig.width = 7.5, fig.height = 6.5}
# TPR in in silico experiments across alpha values 
## Figure 3A: TPR of methods in representative yeast setting at varying alpha levels
datsub <- res5d %>% filter(performanceMetric == "TPR",
                                        abs(alpha-0.10) < 1e-6,
                                        blabel %in% c("adapt-glm", "lfdr", "fdrreg-t")) %>%
                                 group_by(blabel, alpha) %>%
                                 summarize(value = mean(value)) %>%
                                 mutate(Method = blabel) %>%
                                 arrange(desc(Method))
p3a <- plotsim_average(res5d, met="TPR",
                merge_ihw = TRUE, errorBars=TRUE) +
      ggtitle("Resampling with spike-ins") +
      scale_x_continuous(expression(paste(bold(alpha), bold(" level"))),
                         breaks=seq(0, 1, by=0.02), 
                         limits=c(0,0.114)) +
      scale_y_continuous("TPR", labels=scales::percent, breaks=seq(0, 1, by=0.05)) +
      theme(plot.title = element_text(face = "bold", size = 12)) +
      geom_label_repel(data=datsub,
                 aes(alpha, value, label = Method),
                 min.segment.length = unit(0, 'lines'),
                 nudge_x = c(10,10,-0.01), size = 2.25, nudge_y=c(0.008,0.008, 0.008), #l,f,a
                 fill = "white", alpha = 1,
                 box.padding = 0.1, label.padding = 0.1)

## Figure 3B: TPR of methods in representative polyester setting at varying alpha levels
datsub <- res5dp %>% filter(performanceMetric == "TPR",
                                        abs(alpha-0.10) < 1e-6,
                                        blabel %in% c("lfdr", "fdrreg-t")) %>%
                                 group_by(blabel, alpha) %>%
                                 summarize(value = mean(value)) %>%
                                 mutate(Method = blabel) 
p3b <- plotsim_average(res5dp, met="TPR",
                merge_ihw = TRUE, errorBars=TRUE) +
      ggtitle("Polyester count simulation") +
      scale_x_continuous(expression(paste(bold(alpha), bold(" level"))),
                         breaks=seq(0, 1, by=0.02), 
                         limits = c(0,0.114)) +
      scale_y_continuous("", labels=scales::percent, breaks=seq(0, 1, by=0.05)) +
      theme(plot.title = element_text(face = "bold", size = 12)) + 
      theme(legend.position="none") +
      geom_label_repel(data=datsub,
                 aes(alpha, value, label = Method),
                 min.segment.length = unit(0, 'lines'),
                 nudge_x = 10, size = 2.25, 
                 box.padding = 0.1, label.padding = 0.1,
                 fill = "white", alpha = 1)

p3abTitle <- ggdraw() +
    draw_label(expression(paste(bold("TPR in RNA-seq "), 
                                bolditalic("in silico"), bold(" experiments"))))

# Panels C and D -> TPR in sims across settings at a single specified alpha
## Figure 3C: TPR of methods across varying number of tests at alpha.thresh
res_ntests <- dplyr::filter(resmet, setting == "varyingntests", 
                            alpha == alpha.thresh, Method %in% methodset)
p3c <- genplot(res_ntests %>% filter(ntests >= 4000 | Method != "lfdr"),
               cov = ntests, met = "TPR", ebw = 0.1) +
    scale_x_continuous("Number of tests", trans = "log10",
                       breaks = c(1e2, 5e2, 1e3, 5e3, 1e4, 5e4),
                       labels = c("100", "500", "1e3", "5e3", "1e4", "5e4")) +
    guides(color = FALSE, linetype = FALSE) +
    scale_y_continuous("TPR", labels = scales::percent, breaks=seq(0,1,by=0.1)) +
    ggtitle("Varying number of tests")  + 
    geom_line(data = res_ntests %>% 
                filter(ntests <= 5000 & Method == "lfdr", performanceMetric == "TPR"),
              aes(x = ntests, y = mean.info), 
              linetype = "dashed", color = "dodgerblue3", alpha = 0.88) +
    geom_errorbar(data = res_ntests %>% 
                 filter(ntests <= 5000 & Method == "lfdr", performanceMetric == "TPR"),
                 aes(ymin = mean.info - se.info, ymax = mean.info + se.info), width = 0.1, alpha=0.5)

## Figure 3D: FDR of methods across varying null proportion at alpha.thresh
res_pi0 <- dplyr::filter(resmet, setting == "varyingpi0", 
                         alpha == alpha.thresh, Method %in% methodset,
                         pi0 >= 10) # restrict to pi0 at least 10%
p3d <- genplot(res_pi0, cov = 100-pi0, met = "TPR", ebw = 2.5) +
    scale_x_continuous("Proportion non-null",
                       breaks = 100 - c(seq(10, 90, by = 10), 95, 99)) + 
  guides(color = FALSE, linetype = FALSE) + 
  ggtitle("Varying non-null proportion") +
  scale_y_continuous("", labels = scales::percent, breaks=seq(0,1,by=0.20)) 

p3cdTitle <- ggdraw() +
    draw_label(expression(paste(bold("TPR across simulation settings ("), 
                                bold(alpha), bold(" = 0.05)"))))

## pull Figure 3 together
top <- plot_grid(p3a + theme(legend.position="none"), p3b, nrow = 1)
bot <- plot_grid(p3c, p3d, nrow = 1)
Fig3 <- plot_grid(p3abTitle, top, p3cdTitle, bot, nrow=4, 
                  rel_heights = c(0.1, 1, 0.1, 1),
                  labels = c("A", "", "B", ""))
Fig3 <- plot_grid(Fig3, get_legend(p2a), rel_widths = c(1, .2))
Fig3
ggsave(file.path(outdir, "Figure3.pdf"), width=7.5, height=6.5)
```

# Figure 4 - Applicability

This figure shows the applicability of each of the methods to a variety of data types
within the case studies, as well as to different types of statistical tests both 
within the case studies and simulation settings. Panel (A) displays the performance 
across different test statistic distributions, and panel (B) displays the performance
across different case studies. 

```{r Figure4, fig.width = 11, fig.height = 11}
res_noise <- dplyr::filter(resmet, setting == "informative-cubic") %>%
  filter(Method %in% methodset)

## Distribution of unimodal effect sizes (based on random samples)
set.seed(100)
tsdists <- bind_rows(dplyr::mutate(simIteration(1, NULL, m = 2e4, pi0 = 0.9,
                                                es_dist = rnorm_generator(3),
                                                ts_dist = rnorm_perturber(1),
                                                null_dist = rnorm_2pvaluer(1),
                                                icovariate = runif, execute = FALSE),
                                   dist = "\"Distribution: \" * N(0, 1)"),
                     dplyr::mutate(simIteration(1, NULL, m = 2e4, pi0 = 0.9,
                                                es_dist = rnorm_generator(3),
                                                ts_dist = rt_perturber(5),
                                                null_dist = rt_2pvaluer(5),
                                                icovariate = runif, execute = FALSE),
                                   dist = "\"Distribution: \" * t[5]"),
                     dplyr::mutate(simIteration(1, NULL, m = 2e4, pi0 = 0.9,
                                                es_dist = rnorm_generator(3),
                                                ts_dist = rt_perturber(11),
                                                null_dist = rt_2pvaluer(11),
                                                icovariate = runif, execute = FALSE),
                                   dist = "\"Distribution: \" * t[11]"),
                     dplyr::mutate(simIteration(1, NULL, m = 2e4, pi0 = 0.9,
                                                es_dist = function(x) { abs(rnorm_generator(15)(x)) },
                                                ts_dist = rchisq_perturber(4),
                                                null_dist = rchisq_pvaluer(4),
                                                icovariate = runif, execute = FALSE),
                                   dist = "\"Distribution: \" * {\n    chi^2\n}[4]"))
tsdists <- dplyr::mutate(tsdists, dist = factor(dist, levels = levels(factor(dist))[c(2, 3, 4, 1)]))
tsdists <- dplyr::rename(tsdists, truth = qvalue)
tsdists <- dplyr::mutate(tsdists, truth = factor(truth, levels = 0:1, labels = c("null", "non-null")))

## Figure 3A: Performance across distribution of test statistics (based on single replicate)
p4a <- ggplot(tsdists, aes(x = test_statistic, y=..count..,
                            group = truth, color = truth)) +
    stat_density(geom = "line", position = "identity", adjust = 1/2) + 
    theme_classic() +
    scale_color_brewer("Truth", palette = "Set1", direction = -1) + 
    scale_x_continuous("test statistic (single replicate)") +
    coord_cartesian(xlim = c(-5, 25)) + 
    theme(axis.title = element_text(face = "bold", size = 10),
          plot.title = element_text(face = "bold", size = 14)) +
    facet_grid(. ~ dist, labeller = label_parsed) 
p4a <- plot_grid(p4a, genplot(res_noise, cov = alpha, met = "FDR") +
                   facet_grid(. ~ dist, labeller = label_parsed) +
                   scale_x_continuous(expression(paste(bold(alpha), bold(" level"))),
                         breaks=seq(0, 1, by=0.02)) +
                   expand_limits(x = 0) , axis = "trbl", align="hv",
                 nrow=2, rel_heights = c(0.75,1))

Titlep4a <- ggdraw() +
    draw_label(expression(paste(bold("FDR across test statistic distributions in simulation"))))
p4a <- plot_grid(Titlep4a, p4a, nrow = 2, rel_heights = c(0.1,1), labels = c("A", ""))
```



```{r}
## Figure 4B: Case study heatmap demonstrating variability in applicability
## read list of case study results
objects <- list.files(path, recursive=TRUE, pattern="rds", full.names=TRUE)
objects2 <- list.files(path2, recursive=TRUE, pattern="rds", full.names=TRUE)
objects3 <- list.files(path3, recursive=TRUE, pattern="rds", full.names=TRUE)

## exclude yeast and polyester simulations, as well as uninformative covariate
## analyses
objects <- objects[!grepl("yeast", objects) & !grepl("polyester", objects) &
                   !grepl("uninf", objects)]

objects2 <- objects2[!grepl("yeast", objects2) & !grepl("polyester", objects2) &
                   !grepl("uninf", objects2)]

objects3 <- objects3[!grepl("yeast", objects3) & !grepl("polyester", objects3) &
                   !grepl("uninf", objects3)]

## create shorter names for case studies
object_labs <- gsub("\\.rds", "", basename(objects))
object_labs <- gsub("-benchmark", "", object_labs)

usage <- c(1,2)
new_names <- c("adapt-gmm_g",
               "adapt-gmm-neural"
               )


methodset <- c("bh", "ihw", paste0("ihw-a0", 1:9), "ihw-a10",
               "qvalue", "bl", "bl-df03", "adapt-glm",new_names)
# methodset <- c("bh", "ihw", paste0("ihw-a0", 1:9), "ihw-a10",
#              "qvalue", "bl", "bl-df03", "lfdr",
#              "fdrreg-t", "ashq", "adapt-glm",new_names)
## read in case study results (proportion of rejects vs. case study max)
# If this doesn't work, remember to change SummarizedBenchmark version
objdat <- combine_three_tidy_df(objects, objects2, objects3, colLabels=object_labs, 
                                fill="propMaxRejections", annotate=NULL, alpha=alpha.thresh,
                                methodset=methodset,usage=usage)

objdat$method[objdat$method=="New1"] <- new_names[1]
objdat$method[objdat$method=="New2"] <- new_names[2]

  #tidy_df(objects=objects, colLabels=object_labs, 
          #        fill="propMaxRejections", annotate=NULL,
          #        alpha = alpha.thresh)
objdat <- as_tibble(objdat)
objdat <- tidyr::complete(objdat, method, casestudy)



## exclude 'enigma-so4-otus' results with no significant calls
objdat <- dplyr::filter(objdat, !grepl("so4", casestudy))

## create table of case study-to-assay mapping
studies <- tibble(
    casestudy = c("cbp-csaw", "h3k4me3-csaw", "human", "mouse", "bmi-maf", "bmi-samplesize",
                  "enigma-al-otus", "enigma-ph-otus", "enigma-so4-otus", "baxter-genus", 
                  "goodrich-genus", "papa-genus", "schubert-otus",
                  "enigma-al-otus-abun", "enigma-ph-otus-abun", "enigma-so4-otus-abun",
                  "baxter-genus-mean-abun", "goodrich-genus-abun", "papa-genus-abun",
                  "schubert-otus-abun",
                  "brain", "mir200c",
                  "human-mast-det", "human-mast-mean", "human-scdd-det",
                  "human-scdd-mean", "human-wilcox-det", "human-wilcox-mean",
                  "mouse-mast-det", "mouse-mast-mean", "mouse-scdd-det",
                  "mouse-scdd-mean", "mouse-wilcox-det", "mouse-wilcox-mean"),
    assay = c(rep("ChIP-seq", 2), rep("GSEA", 2), rep("GWAS", 2), rep("Microbiome", 14),
              rep("RNA-seq", 2), rep("scRNA-seq", 12)))

## add assay labels to results table
objdat <- left_join(objdat, studies, by = "casestudy")
objdat <- dplyr::filter(objdat, !is.na(assay == "Microbiome"))

# add covariate label to microbiome ubiquity 
objdat <- objdat %>%
          mutate(casestudy = ifelse(assay == "Microbiome", 
                                    ifelse(!grepl("abun", casestudy),
                                           paste0(casestudy, "-ubiq"),
                                           casestudy),
                                    casestudy)) %>%
          mutate(casestudy = gsub("mean-abun", "abun", casestudy))



## create table of rejected proportion for top methods per case study
objdat_props <- dplyr::group_by(objdat, casestudy) %>% filter(method %in% methodset)
objdat_props <- dplyr::filter(objdat_props, nrejects == max(nrejects, na.rm = TRUE))
objdat_props <- dplyr::ungroup(objdat_props)
objdat_props <- dplyr::mutate(objdat_props, lab = paste0(100*round(propPossible, 4), "%"))
objdat_props$plotrow <- "real"

## create table of total tests for each case study
objdat_tab <- dplyr::select(objdat_props, casestudy, nrejects, propPossible, assay)
objdat_tab <- dplyr::distinct(objdat_tab)
objdat_tab <- dplyr::mutate(objdat_tab, `Total Tests` = scales::comma(round(nrejects / propPossible)))
objdat_tab <- dplyr::select(objdat_tab, casestudy, assay, `Total Tests`) 
objdat_tab <- tidyr::gather(objdat_tab, method, cnt, -casestudy, -assay)

## hack to get total counts and heatmap in same plot
objdat_joint <- bind_rows(list(real = objdat, fake = objdat_tab), .id = "plotrow")

# filter for methods in methodset
objdat_joint <- filter(objdat_joint, method %in% c(methodset, "Total Tests"))

## specify method order
method_order <- c("bh", "ihw", "qvalue", "bl", "adapt-glm", new_names, "lfdr", "fdrreg-t", "ashq")
objdat_joint <- dplyr::mutate(objdat_joint, method = factor(method, levels = c(method_order, "Total Tests")))

# add notation for missing because "shouldn't"
objdat_joint <- objdat_joint %>% 
  mutate(should = ifelse(method == "lfdr" & is.na(nrejects), "*", ""))

p4b <- ggplot(objdat_joint, aes(x = casestudy, y = method, fill = propMaxRej)) +
    scale_x_discrete("Case Study", expand = c(0, 0)) +
    scale_y_discrete("Method", expand = c(0, 0)) +
    geom_tile() +
    geom_text(aes(label = lab), data = objdat_props,
              size = 1.8, color = "white") +
    geom_text(aes(label = should), size = 5, color = "black") +
    geom_tile(data = filter(objdat_joint, plotrow == "fake"), fill = "white") + 
    geom_text(aes(label = cnt), data = dplyr::filter(objdat_joint, !is.na(cnt)), size = 2) + 
    scale_fill_distiller("% Rejected\n(relative to max)",
                         palette = "Blues", direction = 1, limits = c(0, 1),
                         labels = scales::percent) +
    facet_grid(plotrow ~ assay, scales = "free", space = "free") +
    theme_bw() + 
    theme(axis.text.x = element_text(angle = 90, vjust = 1/2, hjust = 1),
          strip.text.y = element_blank(),
          legend.position = c(-0.05,-0.3), legend.justification = c(0, 1),
          legend.direction = "horizontal",
          legend.text = element_text(size = 7),
          legend.title = element_text(size = 9),
          legend.margin = margin(6, 10, 6, 10))
Titlep4b <- ggdraw() +
    draw_label(expression(paste(bold("Number of rejections in case studies"))))
p4b <- plot_grid(Titlep4b, p4b, nrow = 2, rel_heights = c(0.075,1), 
                 labels = c("B", ""))
p4b
ggsave(file.path(outdir, "Figure4_neural_comparison.pdf"), width = 11.2, height = 6)
## pull Figure 4 together
# Fig4 <- plot_grid(p4a, p4b, ncol = 1, rel_heights = c(1,1))
# Fig4
# ggsave(file.path(outdir, "Figure4.pdf"), width = 11.2, height = 10.5)
```

# Figure 5 - Consistency

This figure is generated in `manuscript_ratings.Rmd`.

# Figure 6 - Summary

The summary of recommendations figure is drawn manually.

# Session information

```{r}
sessionInfo()
```